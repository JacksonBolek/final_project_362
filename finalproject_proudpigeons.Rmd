---
title: "proudpigeons_finalproject"
author: "Jackson Bolek, Zeke Metz, and Timmy Miller"
date: "4/28/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(caret)
library(magrittr)
library(MLmetrics)
library(fastDummies)
library(DT)
```

# Section 1: Loading and processing the data
```{r }
# read in the data
train<-read_csv("~/final_project_362/final_train.csv")
colnames(train)<-c("Age", "Marital Status", "Education", "Default", "Loan", "Balance", "Month", "Contact Type", "Contact Duration", "Number of Contacts: Current Campaign", "Number of Days Since Contact from Previous Campaign", "Number of Contacts Before this Campaign", "Previous Outcome", "Deposit")

# conver each date from Month/Day to Month only

train$Month<-str_remove(train$Month, "\\s.*")

# make the response variable a factor; leave all other categorical variables as characters for facet_wrap

train$Deposit %<>% as.factor() 

# make dummy variables out of categorical variables

dummy<-dummy_cols(train, c("Marital Status", "Education", "Default", "Month", "Contact Type", "Previous Outcome", "Loan"), remove_selected_columns = T)

# create a data partition for training and test sets; use 75% of the data for training and 25% for testing
set.seed(123)
index<-createDataPartition(train$Age, list=F, p=.75)
train_dummy<-dummy[index,]
test<-dummy[-index,]

# get a glimpse at the distributions of the predictors 
train %>%
  keep(is.character) %>%                     # Keep only character columns
  gather() %>%                             # Convert to key-value pairs
  ggplot(aes(value)) +                     # Plot the values
    facet_wrap(~ key, scales = "free", nrow=4) +   # In separate panels
    geom_bar(fill="red")+labs(title="Categorical Predictors", x="Value", y="Count")+theme(plot.title = element_text(hjust = 0.5))


train %>%
  keep(is.numeric) %>%                     # Keep only numeric columns
  gather() %>%                             # Convert to key-value pairs
  ggplot(aes(value)) +                     # Plot the values
    facet_wrap(~ key, scales = "free", nrow=4) +   # In separate panels
    geom_density(color="blue", fill="blue")+labs(title="Continuous Predictors", x="Value", y="Density")+theme(plot.title = element_text(hjust = 0.5))

# distribution of response
train %>% select(Deposit) %>% ggplot(aes(Deposit))+geom_bar(fill="green")+labs(title="Response Variable", x="Deposit", y="Count")+theme(plot.title = element_text(hjust = 0.5))


```


# Section 2: Building the Models

# defining the trivial model
```{r}
# calculate trivial model log loss; each probability is equal to the probability of "yes" instances in the training data; winds up being about .52
trivial_logloss<-MLmetrics::LogLoss(1-(train_dummy %>% filter(Deposit=="no") %>% nrow()/nrow(train_dummy)), ifelse(train_dummy$Deposit=="yes", 1,0))

# calculate the trivial model accuracy (no information rate); winds up being about .78
trivial_accuracy<-train_dummy %>% filter(Deposit=="no") %>% nrow()/nrow(train_dummy) 


```



#Building the non-trivial models; we will train with 5-fold cross validation and optimize for log loss since log loss is our competition metric

```{r logistic regression}
# no tuning of hyperparameters necessary for logistic regression
set.seed(123)

logreg<-train(data=train_dummy, as.factor(Deposit)~., method="glm", family="binomial", trControl=trainControl("cv", number=5, verbose=T, classProbs=TRUE, summaryFunction=mnLogLoss), metric="logLoss")





```



```{r extreme gradient boosting}

# extreme gradient boosting tunes seven hyperparameters; here we only show the final model, but the hyperparameter tuning was iterative, beginning with eta (learning rate) and nrounds. Then, we moved on to the minimum child weight (minimum number of observations at each terminal node) and then maximum depth of the tree, followed by subsample and colsample_bytree, which provide random samples of the overall training data and the columns, respectively. We then moved on to the penalty term gamma, which wound up being 0. After all of this, we did a final retuning of the eta and nrounds to make sure we were not over or underfitting. 
set.seed(123)
xgb<-train(data=train_dummy, as.factor(Deposit)~., method="xgbTree", trControl=trainControl(method="cv", number=5, verbose=T, classProbs=TRUE, summaryFunction=mnLogLoss), tuneGrid=expand.grid(eta=.01, max_depth=6, gamma=0, colsample_bytree=.8, subsample=.8, nrounds=seq(10,1400,10), min_child_weight=4), metric="logLoss")


# Create presentable data table for variable importance
varImp(xgb)$importance  %>%
  datatable() %>%
  formatRound(columns=c("Overall"), digits=3)





```



```{r Ranger random forest}
#hyperparamter tuning includes the number of predictors available at each node (mtry) and the minimum node size; the best model had mtry=9 and min.node.size=1
set.seed(123)
ranger<-train(data=train_dummy, as.factor(Deposit)~., method="ranger", trControl=trainControl(method="cv", number=5, verbose=T, classProbs=T, summaryFunction=mnLogLoss), tuneGrid=expand.grid(mtry=6:12, min.node.size=c(1,2,3), splitrule="gini"), metric="logLoss")


# create a nice visualization of the hyperparameter tuning for the executive report

ggplot(ranger$results, aes(x=mtry, y=logLoss, color=as.factor(min.node.size)))+geom_line()+labs(title="Tuning Hyperparameters for the Ranger Random Forest Model\n (tuned using the Caret package with 5-fold CV)", x="Number of Predictors Available at Each Node", color="Minimum Node Size", y="Log Loss")+theme(plot.title = element_text(hjust = 0.5))



```



```{r penalized LDA}
# here we only tune the penalty term gamma
set.seed(123)
plda<-train(data=train_dummy, Deposit~., method="pda", trControl=trainControl(method="cv", number=5, verbose=T, classProbs=T, summaryFunction = mnLogLoss), metric="logLoss", tuneGrid=expand.grid(lambda=seq(0, 500, 10)))





```

# Section 3: Testing the Model and Results

```{r}
Model<-c(c("Trivial", "Ranger Random Forest", "Extreme Gradient Boosting", "Penalized LDA", "Logistic Regression")) #model names
Model_object<-list(ranger, xgb, plda, logreg) # model objects to be used in the loop
Accuracy<-vector() 
Sensitivity<-vector()
Specificity<-vector()
LogLoss<-vector()
# use this loop to create a data frame of accuracy, sensitivity, specificity, and log loss for each of the four models we tested, ass well as the trivial model; the first row of the data frame will be the trivial model's statistics. we cannot include the trivial model in the loop because we cannot use the predict() function on a naive model, so instead we manually define the first row and then loop the 2nd through 5th rows (2nd through 5th rows contain the four models that we built)
for (i in 1+seq_along(list(logreg, xgb, ranger, plda))){
  
  Accuracy[1]<-confusionMatrix(factor(rep("no", times=nrow(test)), levels=c("no", "yes")), test$Deposit, positive="yes")$overall[1]
  
  Sensitivity[1]<-confusionMatrix(factor(rep("no", times=nrow(test)), levels=c("no", "yes")), test$Deposit, positive="yes")$byClass[1]
  
  Specificity[1]<-confusionMatrix(factor(rep("no", times=nrow(test)), levels=c("no", "yes")), test$Deposit, positive="yes")$byClass[2]
  
  LogLoss[1]<-MLmetrics::LogLoss((train_dummy %>% filter(Deposit=="yes") %>% nrow()/nrow(train_dummy)), ifelse(train_dummy$Deposit=="yes", 1,0))
  
  Accuracy[i]<-confusionMatrix(predict(Model_object[[i-1]], newdata=test, type="raw"), test$Deposit, positive="yes")$overall[1]
  
  Sensitivity[i]<-confusionMatrix(predict(Model_object[[i-1]], newdata=test, type="raw"), test$Deposit, positive="yes")$byClass[1]
  
  Specificity[i]<-confusionMatrix(predict(Model_object[[i-1]], newdata=test, type="raw"), test$Deposit, positive="yes")$byClass[2]
  
  LogLoss[i]<-LogLoss((predict(Model_object[[i-1]], newdata=test, type="prob")[,2]), ifelse(test$Deposit=="yes",1,0))
  
  
}
# Create data frame with meanigful statistics, use data table to make it presentable
data.frame(Model, Accuracy, Sensitivity, Specificity, LogLoss) %>%
  datatable() %>%
  formatRound(columns=c("Accuracy", "Sensitivity", "Specificity", "LogLoss"), digits=3)





```


# Section 4: Deployment

```{r}
# example of a submission for the competition; follows the same process as reading in the training data, except there is no response to convert to a factor
compete<-read.csv("final_compete.csv") %>% select(-"id")

colnames(compete)<-c("Age", "Marital Status", "Education", "Default", "Loan", "Balance", "Month", "Contact Type", "Contact Duration", "Number of Contacts: Current Campaign", "Number of Days Since Contact from Previous Campaign", "Number of Contacts Before this Campaign", "Previous Outcome")

compete$Month<-str_remove(compete$Month, "\\s.*")


compete<-dummy_cols(compete, c("Marital Status", "Education", "Default", "Month", "Contact Type", "Previous Outcome", "Loan"), remove_selected_columns = T)



pred_compete<-predict(ranger, newdata=compete, type="prob")

# format properly for the competition

compete.final<-data.frame(id=1:nrow(compete), prob=pred_compete[,2]) %>% write_csv(file="proudpigeons_7") #this model had a log loss of .262
```




