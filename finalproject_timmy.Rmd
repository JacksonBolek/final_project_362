---
title: "timmy_finalproject"
author: "Timmy Miller"
date: "4/12/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(caret)
library(magrittr)
library(MLmetrics)
library(fastDummies)
library(gbm)
```


```{r Read notes on feature engineering1!}
train<-read_csv("~/final_project_362/final_train.csv")

train$date<-str_remove(train$date, "\\s.*")

train$deposit %<>% as.factor() 

dummy<-dummy_cols(train, c("marital", "education", "default", "date", "contact_type", "prev_outcome", "loan"), remove_selected_columns = T)

set.seed(123)
index<-createDataPartition(train$age, list=F, p=1)
train_dummy<-dummy[index,]
test<-dummy[-index,]


train %>%
  keep(is.character) %>%                     # Keep only numeric columns
  gather() %>%                             # Convert to key-value pairs
  ggplot(aes(value)) +                     # Plot the values
    facet_wrap(~ key, scales = "free", nrow=4) +   # In separate panels
    geom_bar(fill="red")+labs(title="Categorical Predictors", x="Value", y="Count")+theme(plot.title = element_text(hjust = 0.5))


train %>%
  keep(is.numeric) %>%                     # Keep only numeric columns
  gather() %>%                             # Convert to key-value pairs
  ggplot(aes(value)) +                     # Plot the values
    facet_wrap(~ key, scales = "free", nrow=4) +   # In separate panels
    geom_density(color="blue", fill="blue")+labs(title="Continuous Predictors", x="Value", y="Density")+theme(plot.title = element_text(hjust = 0.5))


train %>% select(deposit) %>% ggplot(aes(deposit))+geom_bar(fill="green")+labs(title="Response Variable", x="Deposit", y="Count")+theme(plot.title = element_text(hjust = 0.5))


```




# defining the trivial model
```{r}
#Log Loss of trivial model is .52, given by Dr. Weber

train_dummy %>% filter(deposit=="no") %>% nrow()/nrow(train_dummy) #accuracy of trivial model is .784


```






#trying with logreg

```{r}
logreg<-train(data=train_dummy, as.factor(deposit)~., method="glm", family="binomial", trControl=trainControl("cv", number=10, verbose=T, classProbs=TRUE, summaryFunction=mnLogLoss), metric="logLoss")

logreg$results

logreg_accuracy<-train(data=train_dummy, as.factor(deposit)~., method="glm", family="binomial", trControl=trainControl("cv", number=10, verbose=T))

logreg_accuracy$results



```


# tring with random forest








# trying with xgboost
```{r}

xgb<-train(data=train_dummy, as.factor(deposit)~., method="xgbTree", trControl=trainControl(method="cv", number=5, verbose=T, classProbs=TRUE, summaryFunction=mnLogLoss), tuneGrid=expand.grid(eta=.01, max_depth=6, gamma=0, colsample_bytree=.8, subsample=.8, nrounds=seq(10,1400,10), min_child_weight=4), metric="logLoss")

xgb$results

varImp(xgb)

xgb_acc<-train(data=train_dummy, as.factor(deposit)~., method="xgbTree", trControl=trainControl(method="cv", number=5, verbose=T), tuneGrid=expand.grid(eta=.01, max_depth=6, gamma=0, colsample_bytree=.8, subsample=.8, nrounds=seq(10,1400,10), min_child_weight=4))

xgb_acc$results

varImp(xgb_acc)




compete<-read.csv("final_compete.csv")

compete$date<-str_remove(compete$date, "\\s.*")
compete<-dummy_cols(compete, c("marital", "education", "default", "date", "contact_type", "prev_outcome", "loan"), remove_selected_columns = T, remove_first_dummy = T)


pred_compete<-predict(xgb, newdata=compete, type="prob")

compete.final<-data.frame(id=1:nrow(compete), prob=pred_compete[,2]) %>% write_csv(file="proudpigeons_5")





```


# trying with the ranger package
```{r}
ranger<-train(data=train_dummy, as.factor(deposit)~., method="ranger", trControl=trainControl(method="cv", number=5, verbose=T, classProbs=T, summaryFunction=mnLogLoss), tuneGrid=expand.grid(mtry=9, min.node.size=c(1), splitrule="gini"), metric="logLoss")

ranger$results

ranger_acc<-train(data=train_dummy, as.factor(deposit)~., method="ranger", trControl=trainControl(method="cv", number=5, verbose=T), tuneGrid=expand.grid(mtry=9, min.node.size=c(1), splitrule="gini"))

ranger_acc$results

pred_ranger<-predict(ranger, newdata=test, type="prob")

ggplot(ranger$results, aes(x=mtry, y=logLoss, color=as.factor(min.node.size)))+geom_line()+labs(title="Tuning Hyperparameters for the Random Forest Model\n (using the Ranger package with 5-fold CV)", x="Number of Predictors Available at Each Node", color="Minimum Node Size", y="Log Loss")+theme(plot.title = element_text(hjust = 0.5))







compete<-read.csv("final_compete.csv")

compete$date<-str_remove(compete$date, "\\s.*")
compete<-dummy_cols(compete, c("marital", "education", "default", "date", "contact_type", "prev_outcome", "loan"), remove_selected_columns = T)


pred_compete<-predict(ranger, newdata=compete, type="prob")

compete.final<-data.frame(id=1:nrow(compete), prob=pred_compete[,2]) %>% write_csv(file="proudpigeons_7")

```



```{r}
plda<-train(data=train_dummy, deposit~., method="pda", trControl=trainControl(method="cv", number=5, verbose=T, classProbs=T, summaryFunction = mnLogLoss), metric="logLoss", tuneGrid=expand.grid(lambda=seq(0, 500, 10)))

plda$results

plda_acc<-train(data=train_dummy, deposit~., method="pda", trControl=trainControl(method="cv", number=5, verbose=T), tuneGrid=expand.grid(lambda=seq(0, 500, 10)))

plda_acc$results

ggplot(plda$results, aes(lambda, logLoss))+geom_line(color="pink")



```

