---
title: "timmy_finalproject"
author: "Timmy Miller"
date: "4/12/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(caret)
library(magrittr)
library(MLmetrics)
library(fastDummies)
library(gbm)
```


```{r Read notes on feature engineering1!}
train<-read_csv("final_train.csv")

train$date<-str_remove(train$date, "\\s.*")

train$deposit %<>% as.factor() 

dummy<-dummy_cols(train, c("marital", "education", "default", "date", "contact_type", "prev_outcome", "loan"), remove_selected_columns = T)

set.seed(123)
index<-createDataPartition(train$age, list=F, p=1)
train_dummy<-dummy[index,]
test<-dummy[-index,]




```










#trying with logreg

```{r}
logreg<-train(data=train_dummy, as.factor(deposit)~., method="glm", family="binomial", trControl=trainControl("cv", number=10, verbose=T))


pred_logreg<-predict(logreg, newdata=test, type="prob")

MLmetrics::LogLoss(pred_logreg[,2], test$deposit)

logreg_logs<-train(data=train_dummy_logs, as.factor(deposit)~., method="glm", family="binomial", trControl=trainControl("cv", number=10, verbose=T))


pred_logreg_logs<-predict(logreg_logs, newdata=test, type="prob")

MLmetrics::LogLoss(pred_logreg_logs[,2], test$deposit)

```


# tring with random forest

```{r}
set.seed(123)
rf<-train(data=train_dummy, as.factor(deposit)~., method="rf", trControl=trainControl("cv", number=10, verbose=T), ntree=200, tuneGrid=expand.grid(mtry=8))

rf$bestTune

pred_rf<-predict(rf, newdata=test, type="prob")

raw_rf<-predict(rf, newdata=test, type="raw")

confusionMatrix(raw_rf, as.factor(test$deposit))

MLmetrics::LogLoss(pred_rf[,2], test$deposit)

pred_rf[,2]


varImp(rf)

trivial_model<-rep((train$deposit %>% mean()), times=length(test$deposit))



MLmetrics::LogLoss(trivial_model, test$deposit)


compete<-read.csv("final_compete.csv")

compete$date<-str_remove(compete$date, "\\s.*")


compete<-dummy_cols(compete, c("marital", "education", "default", "no", "date", "contact_type", "prev_outcome", "loan"), remove_selected_columns = T, remove_first_dummy = T)


pred_compete<-predict(rf, newdata=compete, type="prob")

compete.final<-data.frame(id=1:nrow(compete), prob=pred_compete[,2]) %>% write_csv(file="proudpigeons_compete")

```




# trying with xgboost
```{r}
# make into factor just to see
train_dummy$deposit<-factor(ifelse(train_dummy$deposit==1, "yes", "no"))
xgb<-train(data=train_dummy, as.factor(deposit)~., method="xgbTree", trControl=trainControl(method="cv", number=5, verbose=T, classProbs=TRUE, summaryFunction=mnLogLoss), tuneGrid=expand.grid(eta=.1, max_depth=seq(5,15, 2), gamma=seq(0.05, .15, .02), colsample_bytree=.6, subsample=1, nrounds=100, min_child_weight=c(1:3)), metric="logLoss")

xgb$bestTune #nrounds=100, max_depth=10, eta=.1, gamma=.1, colsample_bytree=.6, subsample=1, min_child_weight=1

pred_xgb<-predict(xgb, newdata=test, type="prob")


MLmetrics::LogLoss(pred_xgb[,2], test$deposit)


```

```{r}

compete<-read.csv("final_compete.csv")

compete$date<-str_remove(compete$date, "\\s.*")
compete<-dummy_cols(compete, c("marital", "education", "default", "date", "contact_type", "prev_outcome", "loan"), remove_selected_columns = T, remove_first_dummy = T)


pred_compete<-predict(xgb, newdata=compete, type="prob")

compete.final<-data.frame(id=1:nrow(compete), prob=pred_compete[,2]) %>% write_csv(file="proudpigeons_2")


train<-read_csv("final_train.csv")

train$date<-str_remove(train$date, "\\s.*")
train$deposit %<>% as.factor() 

train_dummy<-dummy_cols(train, c("marital", "education", "default", "date", "contact_type", "prev_outcome", "loan"), remove_selected_columns = T, remove_first_dummy = T)

```



# trying with feature engineering  (see the code starting at line 22)
```{r}

xgb<-train(data=train_dummy, as.factor(deposit)~., method="xgbTree", trControl=trainControl(method="cv", number=5, verbose=T, classProbs=TRUE, summaryFunction=mnLogLoss), tuneGrid=expand.grid(eta=.01, max_depth=6, gamma=0, colsample_bytree=.8, subsample=.8, nrounds=seq(10,1400,10), min_child_weight=4), metric="logLoss")

xgb$results

pred_xgb<-predict(xgb, newdata=test, type="prob")


MLmetrics::LogLoss(pred_xgb[,2], ifelse(test$deposit=="yes", 1, 0))

varImp(xgb)


confusionMatrix(predict(xgb, newdata=test, type="raw"), test$deposit)


compete<-read.csv("final_compete.csv")

compete$date<-str_remove(compete$date, "\\s.*")
compete<-dummy_cols(compete, c("marital", "education", "default", "date", "contact_type", "prev_outcome", "loan"), remove_selected_columns = T, remove_first_dummy = T)


pred_compete<-predict(xgb, newdata=compete, type="prob")

compete.final<-data.frame(id=1:nrow(compete), prob=pred_compete[,2]) %>% write_csv(file="proudpigeons_5")



# Fitting nrounds = 100, max_depth = 10, eta = 0.1, gamma = 1, colsample_bytree = 0.6, min_child_weight = 1, subsample = 1 on full training set

```


# trying with the ranger package
```{r}
ranger<-train(data=train_dummy, as.factor(deposit)~., method="ranger", trControl=trainControl(method="cv", number=10, verbose=T, classProbs=T, summaryFunction=mnLogLoss), tuneGrid=expand.grid(mtry=6:9, min.node.size=c(1,2,3), splitrule="gini"), metric="logLoss")

ranger$results %>% filter(logLoss==min(logLoss))

pred_ranger<-predict(ranger, newdata=test, type="prob")


# MLmetrics::LogLoss(pred_ranger[,2], ifelse(test$deposit=="yes",1,0))


compete<-read.csv("final_compete.csv")

compete$date<-str_remove(compete$date, "\\s.*")
compete<-dummy_cols(compete, c("marital", "education", "default", "date", "contact_type", "prev_outcome", "loan"), remove_selected_columns = T)


pred_compete<-predict(ranger, newdata=compete, type="prob")

compete.final<-data.frame(id=1:nrow(compete), prob=pred_compete[,2]) %>% write_csv(file="proudpigeons_7")

```



```{r}
plda<-train(data=train_dummy, deposit~., method="pda", trControl=trainControl(method="cv", number=5, verbose=T, classProbs=T, summaryFunction = mnLogLoss), metric="logLoss", tuneGrid=expand.grid(lambda=seq(200, 300, 10)))

pred_plda<-predict(plda, newdata=test, type="prob")


MLmetrics::LogLoss(pred_plda[,2], ifelse(test$deposit=="yes", 1, 0))

```

```{r}
qrf<-train(data=train_dummy, deposit~., method="ORFlog", trControl=trainControl(method="cv", number=5, verbose=T, classProbs=T, summaryFunction = mnLogLoss), metric="logLoss", tuneControl=expand.grid(mtry=1:10))

pred_qrf<-predict(qrf, newdata=test, type="prob")



MLmetrics::LogLoss(pred_qrf[,2], ifelse(test$deposit=="yes", 1, 0))

```